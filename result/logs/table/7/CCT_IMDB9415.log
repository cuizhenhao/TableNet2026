2024-09-04 15:40:21,298 : CCT_IMDB(
2024-09-04 15:40:21,298 :   (tokenizer): Tokenizer(
2024-09-04 15:40:21,298 :     (emb_conv): WhiteConvEmbedding(
2024-09-04 15:40:21,298 :       (conv): Conv2d(1, 64, kernel_size=(1, 50), stride=(1, 1), bias=False, padding_mode=replicate)
2024-09-04 15:40:21,298 :       (embedding): Embedding(10001, 50)
2024-09-04 15:40:21,298 :     )
2024-09-04 15:40:21,299 :     (pool): WhiteMaxPool()
2024-09-04 15:40:21,299 :     (transpose): WhiteTranspose()
2024-09-04 15:40:21,299 :   )
2024-09-04 15:40:21,299 :   (classifier): TransformerClassifier(
2024-09-04 15:40:21,299 :     (positional_emb): WhiteParameterAdd()
2024-09-04 15:40:21,299 :     (blocks): WhiteSequential(
2024-09-04 15:40:21,299 :       (0): TransformerEncoderLayer(
2024-09-04 15:40:21,299 :         (pre_norm): WhiteLayerNorm((64,), eps=1e-05, elementwise_affine=True)
2024-09-04 15:40:21,299 :         (self_attn): Attention(
2024-09-04 15:40:21,299 :           (qkv): WhiteLinear(in_features=64, out_features=192, bias=False)
2024-09-04 15:40:21,299 :           (proj): WhiteLinear(in_features=64, out_features=64, bias=True)
2024-09-04 15:40:21,299 :         )
2024-09-04 15:40:21,299 :         (self_attention): WhiteMultiHeadAttention(
2024-09-04 15:40:21,299 :           (head1): WhiteAttention(
2024-09-04 15:40:21,299 :             (nonzero): NoneZeroActivationFunction()
2024-09-04 15:40:21,299 :             (q): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:40:21,299 :             (k): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:40:21,299 :             (v): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:40:21,299 :             (transpose): WhiteTranspose()
2024-09-04 15:40:21,299 :             (matmul): WhiteMatMul()
2024-09-04 15:40:21,299 :             (softmax): WhiteSoftmax()
2024-09-04 15:40:21,299 :             (matmul2): WhiteMatMulPro()
2024-09-04 15:40:21,299 :           )
2024-09-04 15:40:21,299 :           (head2): WhiteAttention(
2024-09-04 15:40:21,299 :             (nonzero): NoneZeroActivationFunction()
2024-09-04 15:40:21,299 :             (q): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:40:21,299 :             (k): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:40:21,299 :             (v): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:40:21,299 :             (transpose): WhiteTranspose()
2024-09-04 15:40:21,299 :             (matmul): WhiteMatMul()
2024-09-04 15:40:21,299 :             (softmax): WhiteSoftmax()
2024-09-04 15:40:21,299 :             (matmul2): WhiteMatMulPro()
2024-09-04 15:40:21,299 :           )
2024-09-04 15:40:21,299 :           (concat): WhiteCat2D()
2024-09-04 15:40:21,299 :           (proj): WhiteLinear(in_features=64, out_features=64, bias=True)
2024-09-04 15:40:21,299 :         )
2024-09-04 15:40:21,299 :         (add1): WhiteAdd()
2024-09-04 15:40:21,299 :         (norm1): WhiteLayerNorm((64,), eps=1e-05, elementwise_affine=True)
2024-09-04 15:40:21,299 :         (linear1): WhiteLinear(in_features=64, out_features=32, bias=True)
2024-09-04 15:40:21,299 :         (linear2): WhiteLinear(in_features=32, out_features=64, bias=True)
2024-09-04 15:40:21,299 :         (add2): WhiteAdd()
2024-09-04 15:40:21,299 :       )
2024-09-04 15:40:21,299 :     )
2024-09-04 15:40:21,300 :     (norm): WhiteLayerNorm((64,), eps=1e-05, elementwise_affine=True)
2024-09-04 15:40:21,300 :     (attention_pool): WhiteLinear(in_features=64, out_features=1, bias=True)
2024-09-04 15:40:21,300 :     (softmax): WhiteSoftmax()
2024-09-04 15:40:21,300 :     (matmul): WhiteMatMulPro()
2024-09-04 15:40:21,300 :     (fc): WhiteLinear(in_features=64, out_features=1, bias=True)
2024-09-04 15:40:21,300 :   )
2024-09-04 15:40:21,300 : )
2024-09-04 15:40:21,427 : 0 125 Acc: 42.000%
2024-09-04 15:40:21,452 : 1 125 Acc: 46.500%
2024-09-04 15:40:21,475 : 2 125 Acc: 47.333%
2024-09-04 15:40:21,497 : 3 125 Acc: 49.000%
2024-09-04 15:40:21,524 : 4 125 Acc: 49.200%
2024-09-04 15:40:21,543 : 5 125 Acc: 50.000%
2024-09-04 15:40:21,562 : 6 125 Acc: 49.286%
2024-09-04 15:40:21,580 : 7 125 Acc: 49.250%
2024-09-04 15:40:21,596 : 8 125 Acc: 49.556%
2024-09-04 15:40:21,613 : 9 125 Acc: 50.700%
2024-09-04 15:40:21,630 : 10 125 Acc: 50.909%
2024-09-04 15:40:21,647 : 11 125 Acc: 51.000%
2024-09-04 15:40:21,664 : 12 125 Acc: 50.923%
2024-09-04 15:40:21,681 : 13 125 Acc: 51.000%
2024-09-04 15:40:21,700 : 14 125 Acc: 50.733%
2024-09-04 15:40:21,719 : 15 125 Acc: 51.000%
2024-09-04 15:40:21,739 : 16 125 Acc: 51.235%
2024-09-04 15:40:21,758 : 17 125 Acc: 50.944%
2024-09-04 15:40:21,777 : 18 125 Acc: 51.000%
2024-09-04 15:40:21,795 : 19 125 Acc: 51.150%
2024-09-04 15:40:21,812 : 20 125 Acc: 51.667%
2024-09-04 15:40:21,828 : 21 125 Acc: 51.727%
2024-09-04 15:40:21,845 : 22 125 Acc: 52.043%
2024-09-04 15:40:21,861 : 23 125 Acc: 51.708%
2024-09-04 15:40:21,878 : 24 125 Acc: 51.720%
2024-09-04 15:40:21,896 : 25 125 Acc: 51.577%
2024-09-04 15:40:21,917 : 26 125 Acc: 51.667%
2024-09-04 15:40:21,939 : 27 125 Acc: 51.750%
2024-09-04 15:40:21,960 : 28 125 Acc: 51.759%
2024-09-04 15:40:21,981 : 29 125 Acc: 51.400%
2024-09-04 15:40:21,999 : 30 125 Acc: 51.290%
2024-09-04 15:40:22,017 : 31 125 Acc: 51.156%
2024-09-04 15:40:22,033 : 32 125 Acc: 51.091%
2024-09-04 15:40:22,049 : 33 125 Acc: 51.059%
2024-09-04 15:40:22,066 : 34 125 Acc: 51.171%
2024-09-04 15:40:22,083 : 35 125 Acc: 51.361%
2024-09-04 15:40:22,099 : 36 125 Acc: 51.351%
2024-09-04 15:40:22,118 : 37 125 Acc: 51.605%
2024-09-04 15:40:22,136 : 38 125 Acc: 51.308%
2024-09-04 15:40:22,155 : 39 125 Acc: 51.100%
2024-09-04 15:40:22,173 : 40 125 Acc: 50.878%
2024-09-04 15:40:22,192 : 41 125 Acc: 50.976%
2024-09-04 15:40:22,211 : 42 125 Acc: 50.791%
2024-09-04 15:40:22,228 : 43 125 Acc: 50.818%
2024-09-04 15:40:22,244 : 44 125 Acc: 50.867%
2024-09-04 15:40:22,260 : 45 125 Acc: 50.783%
2024-09-04 15:40:22,277 : 46 125 Acc: 50.830%
2024-09-04 15:40:22,294 : 47 125 Acc: 50.896%
2024-09-04 15:40:22,311 : 48 125 Acc: 50.959%
2024-09-04 15:40:22,331 : 49 125 Acc: 50.860%
2024-09-04 15:40:22,349 : 50 125 Acc: 50.765%
2024-09-04 15:40:22,369 : 51 125 Acc: 50.788%
2024-09-04 15:40:22,387 : 52 125 Acc: 50.755%
2024-09-04 15:40:22,405 : 53 125 Acc: 50.537%
2024-09-04 15:40:22,423 : 54 125 Acc: 50.418%
2024-09-04 15:40:22,442 : 55 125 Acc: 50.518%
2024-09-04 15:40:22,458 : 56 125 Acc: 50.649%
2024-09-04 15:40:22,476 : 57 125 Acc: 50.500%
2024-09-04 15:40:22,492 : 58 125 Acc: 50.441%
2024-09-04 15:40:22,508 : 59 125 Acc: 50.467%
2024-09-04 15:40:22,526 : 60 125 Acc: 50.443%
2024-09-04 15:40:22,545 : 61 125 Acc: 50.403%
2024-09-04 15:40:22,563 : 62 125 Acc: 50.349%
2024-09-04 15:40:22,582 : 63 125 Acc: 50.312%
2024-09-04 15:40:22,601 : 64 125 Acc: 50.385%
2024-09-04 15:40:22,620 : 65 125 Acc: 50.424%
2024-09-04 15:40:22,637 : 66 125 Acc: 50.418%
2024-09-04 15:40:22,655 : 67 125 Acc: 50.441%
2024-09-04 15:40:22,672 : 68 125 Acc: 50.449%
2024-09-04 15:40:22,689 : 69 125 Acc: 50.457%
2024-09-04 15:40:22,706 : 70 125 Acc: 50.451%
2024-09-04 15:40:22,722 : 71 125 Acc: 50.403%
2024-09-04 15:40:22,739 : 72 125 Acc: 50.342%
2024-09-04 15:40:22,760 : 73 125 Acc: 50.230%
2024-09-04 15:40:22,780 : 74 125 Acc: 50.227%
2024-09-04 15:40:22,798 : 75 125 Acc: 50.237%
2024-09-04 15:40:22,817 : 76 125 Acc: 50.208%
2024-09-04 15:40:22,836 : 77 125 Acc: 50.192%
2024-09-04 15:40:22,853 : 78 125 Acc: 50.291%
2024-09-04 15:40:22,877 : 79 125 Acc: 50.275%
2024-09-04 15:40:22,895 : 80 125 Acc: 50.259%
2024-09-04 15:40:22,912 : 81 125 Acc: 50.354%
2024-09-04 15:40:22,930 : 82 125 Acc: 50.422%
2024-09-04 15:40:22,948 : 83 125 Acc: 50.286%
2024-09-04 15:40:22,967 : 84 125 Acc: 50.212%
2024-09-04 15:40:22,986 : 85 125 Acc: 50.233%
2024-09-04 15:40:23,005 : 86 125 Acc: 50.230%
2024-09-04 15:40:23,023 : 87 125 Acc: 50.193%
2024-09-04 15:40:23,043 : 88 125 Acc: 50.169%
2024-09-04 15:40:23,061 : 89 125 Acc: 50.167%
2024-09-04 15:40:23,080 : 90 125 Acc: 50.176%
2024-09-04 15:40:23,099 : 91 125 Acc: 50.207%
2024-09-04 15:40:23,118 : 92 125 Acc: 50.129%
2024-09-04 15:40:23,137 : 93 125 Acc: 50.160%
2024-09-04 15:40:23,156 : 94 125 Acc: 50.042%
2024-09-04 15:40:23,174 : 95 125 Acc: 49.979%
2024-09-04 15:40:23,193 : 96 125 Acc: 50.021%
2024-09-04 15:40:23,212 : 97 125 Acc: 50.071%
2024-09-04 15:40:23,231 : 98 125 Acc: 50.020%
2024-09-04 15:40:23,251 : 99 125 Acc: 50.090%
2024-09-04 15:40:23,271 : 100 125 Acc: 50.059%
2024-09-04 15:40:23,290 : 101 125 Acc: 50.049%
2024-09-04 15:40:23,309 : 102 125 Acc: 50.126%
2024-09-04 15:40:23,328 : 103 125 Acc: 50.154%
2024-09-04 15:40:23,347 : 104 125 Acc: 50.105%
2024-09-04 15:40:23,377 : 105 125 Acc: 50.057%
2024-09-04 15:40:23,395 : 106 125 Acc: 50.159%
2024-09-04 15:40:23,411 : 107 125 Acc: 50.093%
2024-09-04 15:40:23,428 : 108 125 Acc: 50.174%
2024-09-04 15:40:23,445 : 109 125 Acc: 50.164%
2024-09-04 15:40:23,460 : 110 125 Acc: 50.153%
2024-09-04 15:40:23,477 : 111 125 Acc: 50.170%
2024-09-04 15:40:23,494 : 112 125 Acc: 50.186%
2024-09-04 15:40:23,510 : 113 125 Acc: 50.184%
2024-09-04 15:40:23,527 : 114 125 Acc: 50.191%
2024-09-04 15:40:23,543 : 115 125 Acc: 50.216%
2024-09-04 15:40:23,560 : 116 125 Acc: 50.222%
2024-09-04 15:40:23,577 : 117 125 Acc: 50.220%
2024-09-04 15:40:23,593 : 118 125 Acc: 50.227%
2024-09-04 15:40:23,610 : 119 125 Acc: 50.292%
2024-09-04 15:40:23,627 : 120 125 Acc: 50.347%
2024-09-04 15:40:23,643 : 121 125 Acc: 50.385%
2024-09-04 15:40:23,661 : 122 125 Acc: 50.350%
2024-09-04 15:40:23,677 : 123 125 Acc: 50.266%
2024-09-04 15:40:23,695 : 124 125 Acc: 50.232%
2024-09-04 15:41:02,442 : CCT_IMDB(
2024-09-04 15:41:02,442 :   (tokenizer): Tokenizer(
2024-09-04 15:41:02,442 :     (emb_conv): WhiteConvEmbedding(
2024-09-04 15:41:02,442 :       (conv): Conv2d(1, 64, kernel_size=(1, 50), stride=(1, 1), bias=False, padding_mode=replicate)
2024-09-04 15:41:02,442 :       (embedding): Embedding(10001, 50)
2024-09-04 15:41:02,442 :     )
2024-09-04 15:41:02,442 :     (pool): WhiteMaxPool()
2024-09-04 15:41:02,442 :     (transpose): WhiteTranspose()
2024-09-04 15:41:02,442 :   )
2024-09-04 15:41:02,442 :   (classifier): TransformerClassifier(
2024-09-04 15:41:02,442 :     (positional_emb): WhiteParameterAdd()
2024-09-04 15:41:02,442 :     (blocks): WhiteSequential(
2024-09-04 15:41:02,443 :       (0): TransformerEncoderLayer(
2024-09-04 15:41:02,443 :         (pre_norm): WhiteLayerNorm((64,), eps=1e-05, elementwise_affine=True)
2024-09-04 15:41:02,443 :         (self_attn): Attention(
2024-09-04 15:41:02,443 :           (qkv): WhiteLinear(in_features=64, out_features=192, bias=False)
2024-09-04 15:41:02,443 :           (proj): WhiteLinear(in_features=64, out_features=64, bias=True)
2024-09-04 15:41:02,443 :         )
2024-09-04 15:41:02,443 :         (self_attention): WhiteMultiHeadAttention(
2024-09-04 15:41:02,443 :           (head1): WhiteAttention(
2024-09-04 15:41:02,443 :             (nonzero): NoneZeroActivationFunction()
2024-09-04 15:41:02,443 :             (q): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:41:02,443 :             (k): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:41:02,443 :             (v): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:41:02,443 :             (transpose): WhiteTranspose()
2024-09-04 15:41:02,443 :             (matmul): WhiteMatMul()
2024-09-04 15:41:02,443 :             (softmax): WhiteSoftmax()
2024-09-04 15:41:02,443 :             (matmul2): WhiteMatMulPro()
2024-09-04 15:41:02,443 :           )
2024-09-04 15:41:02,443 :           (head2): WhiteAttention(
2024-09-04 15:41:02,443 :             (nonzero): NoneZeroActivationFunction()
2024-09-04 15:41:02,443 :             (q): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:41:02,443 :             (k): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:41:02,443 :             (v): WhiteLinear(in_features=64, out_features=32, bias=False)
2024-09-04 15:41:02,443 :             (transpose): WhiteTranspose()
2024-09-04 15:41:02,443 :             (matmul): WhiteMatMul()
2024-09-04 15:41:02,443 :             (softmax): WhiteSoftmax()
2024-09-04 15:41:02,443 :             (matmul2): WhiteMatMulPro()
2024-09-04 15:41:02,443 :           )
2024-09-04 15:41:02,443 :           (concat): WhiteCat2D()
2024-09-04 15:41:02,443 :           (proj): WhiteLinear(in_features=64, out_features=64, bias=True)
2024-09-04 15:41:02,443 :         )
2024-09-04 15:41:02,443 :         (add1): WhiteAdd()
2024-09-04 15:41:02,443 :         (norm1): WhiteLayerNorm((64,), eps=1e-05, elementwise_affine=True)
2024-09-04 15:41:02,443 :         (linear1): WhiteLinear(in_features=64, out_features=32, bias=True)
2024-09-04 15:41:02,443 :         (linear2): WhiteLinear(in_features=32, out_features=64, bias=True)
2024-09-04 15:41:02,443 :         (add2): WhiteAdd()
2024-09-04 15:41:02,443 :       )
2024-09-04 15:41:02,443 :     )
2024-09-04 15:41:02,443 :     (norm): WhiteLayerNorm((64,), eps=1e-05, elementwise_affine=True)
2024-09-04 15:41:02,443 :     (attention_pool): WhiteLinear(in_features=64, out_features=1, bias=True)
2024-09-04 15:41:02,444 :     (softmax): WhiteSoftmax()
2024-09-04 15:41:02,444 :     (matmul): WhiteMatMulPro()
2024-09-04 15:41:02,444 :     (fc): WhiteLinear(in_features=64, out_features=1, bias=True)
2024-09-04 15:41:02,444 :   )
2024-09-04 15:41:02,444 : )
2024-09-04 15:41:02,486 : ==============================1-tokenizer.emb_conv load==============================
2024-09-04 15:41:02,503 : ==============================2-tokenizer.pool load==============================
2024-09-04 15:41:02,511 : ==============================3-tokenizer.transpose load==============================
2024-09-04 15:41:02,516 : ==============================4-classifier.positional_emb load==============================
2024-09-04 15:41:02,621 : ==============================5-classifier.blocks.0.self_attention.head1.q load==============================
2024-09-04 15:41:02,735 : ==============================6-classifier.blocks.0.self_attention.head1.k load==============================
2024-09-04 15:41:02,841 : ==============================7-classifier.blocks.0.self_attention.head1.v load==============================
2024-09-04 15:41:02,842 : ==============================8-classifier.blocks.0.self_attention.head1.matmul load==============================
2024-09-04 15:41:02,843 : ==============================9-classifier.blocks.0.self_attention.head1.softmax load==============================
2024-09-04 15:41:02,847 : ==============================10-classifier.blocks.0.self_attention.head1.matmul2 load==============================
2024-09-04 15:41:02,960 : ==============================11-classifier.blocks.0.self_attention.head2.q load==============================
2024-09-04 15:41:03,074 : ==============================12-classifier.blocks.0.self_attention.head2.k load==============================
2024-09-04 15:41:03,184 : ==============================13-classifier.blocks.0.self_attention.head2.v load==============================
2024-09-04 15:41:03,186 : ==============================14-classifier.blocks.0.self_attention.head2.matmul load==============================
2024-09-04 15:41:03,188 : ==============================15-classifier.blocks.0.self_attention.head2.softmax load==============================
2024-09-04 15:41:03,194 : ==============================16-classifier.blocks.0.self_attention.head2.matmul2 load==============================
2024-09-04 15:41:03,195 : ==============================17-classifier.blocks.0.self_attention.concat load==============================
2024-09-04 15:41:03,542 : ==============================18-classifier.blocks.0.self_attention.proj load==============================
2024-09-04 15:41:03,544 : ==============================19-classifier.blocks.0.add1 load==============================
2024-09-04 15:41:03,669 : ==============================20-classifier.blocks.0.linear1 load==============================
2024-09-04 15:41:03,786 : ==============================21-classifier.blocks.0.linear2 load==============================
2024-09-04 15:41:03,788 : ==============================22-classifier.blocks.0.add2 load==============================
2024-09-04 15:41:03,793 : ==============================23-classifier.attention_pool load==============================
2024-09-04 15:41:03,794 : ==============================24-classifier.softmax load==============================
2024-09-04 15:41:03,800 : ==============================25-classifier.matmul load==============================
2024-09-04 15:41:03,803 : ==============================26-classifier.fc load==============================
2024-09-04 15:41:07,799 : 0 125 Acc: 86.000%
2024-09-04 15:41:11,861 : 1 125 Acc: 87.500%
2024-09-04 15:41:16,048 : 2 125 Acc: 88.000%
2024-09-04 15:41:20,139 : 3 125 Acc: 87.750%
2024-09-04 15:41:24,326 : 4 125 Acc: 87.400%
2024-09-04 15:41:28,428 : 5 125 Acc: 87.333%
2024-09-04 15:41:32,525 : 6 125 Acc: 87.286%
2024-09-04 15:41:36,539 : 7 125 Acc: 87.375%
2024-09-04 15:41:40,677 : 8 125 Acc: 86.444%
2024-09-04 15:41:44,776 : 9 125 Acc: 86.700%
2024-09-04 15:41:48,899 : 10 125 Acc: 87.182%
2024-09-04 15:41:53,103 : 11 125 Acc: 87.333%
2024-09-04 15:41:57,253 : 12 125 Acc: 87.308%
2024-09-04 15:42:01,492 : 13 125 Acc: 87.357%
2024-09-04 15:42:05,553 : 14 125 Acc: 87.333%
2024-09-04 15:42:09,574 : 15 125 Acc: 87.312%
2024-09-04 15:42:13,570 : 16 125 Acc: 87.647%
2024-09-04 15:42:17,791 : 17 125 Acc: 87.833%
2024-09-04 15:42:21,836 : 18 125 Acc: 87.842%
2024-09-04 15:42:25,977 : 19 125 Acc: 87.950%
2024-09-04 15:42:30,101 : 20 125 Acc: 88.238%
2024-09-04 15:42:34,116 : 21 125 Acc: 88.273%
2024-09-04 15:42:38,331 : 22 125 Acc: 88.522%
2024-09-04 15:42:42,364 : 23 125 Acc: 88.500%
2024-09-04 15:42:46,472 : 24 125 Acc: 88.480%
2024-09-04 15:42:50,960 : 25 125 Acc: 88.654%
2024-09-04 15:42:55,050 : 26 125 Acc: 88.593%
2024-09-04 15:42:59,513 : 27 125 Acc: 88.643%
2024-09-04 15:43:03,814 : 28 125 Acc: 88.759%
2024-09-04 15:43:07,929 : 29 125 Acc: 88.733%
2024-09-04 15:43:11,935 : 30 125 Acc: 88.903%
2024-09-04 15:43:13,835 : Traceback (most recent call last):
2024-09-04 15:43:13,835 :   File "E:\TableNet\generateWhitebox.py", line 170, in <module>
2024-09-04 15:43:13,835 :     net.test_whitebox(save_path, train_set, test_set, test_batch_size, clustering_batch_size, white_bit, input_format)
2024-09-04 15:43:13,835 :   File "E:\TableNet\modules\white_net.py", line 85, in test_whitebox
2024-09-04 15:43:13,835 :     self.test_once(test_loader)
2024-09-04 15:43:13,835 :   File "E:\TableNet\modules\white_net.py", line 26, in test_once
2024-09-04 15:43:13,835 :     outputs = self(inputs)
2024-09-04 15:43:13,835 :   File "D:\Projects\Pycharm\WBML_update(1)\venv\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
2024-09-04 15:43:13,835 :     return forward_call(*input, **kwargs)
2024-09-04 15:43:13,835 :   File "E:\TableNet\net\cct_imdb.py", line 326, in forward
2024-09-04 15:43:13,836 :     return self.classifier(x)
2024-09-04 15:43:13,836 :   File "D:\Projects\Pycharm\WBML_update(1)\venv\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
2024-09-04 15:43:13,836 :     return forward_call(*input, **kwargs)
2024-09-04 15:43:13,836 :   File "E:\TableNet\net\cct_imdb.py", line 259, in forward
2024-09-04 15:43:13,836 :     x = blk(x)
2024-09-04 15:43:13,836 :   File "D:\Projects\Pycharm\WBML_update(1)\venv\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
2024-09-04 15:43:13,836 :     return forward_call(*input, **kwargs)
2024-09-04 15:43:13,836 :   File "E:\TableNet\net\cct_imdb.py", line 207, in forward
2024-09-04 15:43:13,836 :     x = self.self_attention(src)
2024-09-04 15:43:13,836 :   File "D:\Projects\Pycharm\WBML_update(1)\venv\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
2024-09-04 15:43:13,837 :     return forward_call(*input, **kwargs)
2024-09-04 15:43:13,837 :   File "E:\TableNet\net\cct_imdb.py", line 137, in forward
2024-09-04 15:43:13,837 :     x1 = self.head1(x)
2024-09-04 15:43:13,837 :   File "D:\Projects\Pycharm\WBML_update(1)\venv\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
2024-09-04 15:43:13,837 :     return forward_call(*input, **kwargs)
2024-09-04 15:43:13,837 :   File "E:\TableNet\net\cct_imdb.py", line 92, in forward
2024-09-04 15:43:13,837 :     q = self.q(x)
2024-09-04 15:43:13,837 :   File "D:\Projects\Pycharm\WBML_update(1)\venv\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
2024-09-04 15:43:13,837 :     return forward_call(*input, **kwargs)
2024-09-04 15:43:13,838 :   File "E:\TableNet\modules\parents\clustering_layer.py", line 88, in forward
2024-09-04 15:43:13,838 :     out = self._whitebox_forward(input)
2024-09-04 15:43:13,838 :   File "E:\TableNet\modules\white_linear.py", line 35, in _whitebox_forward
2024-09-04 15:43:13,838 :     out[..., i] = table[out[..., i], input_arr[..., index]]
2024-09-04 15:43:13,838 : KeyboardInterrupt
